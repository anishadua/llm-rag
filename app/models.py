# app/models.py

from datetime import datetime
from typing import List, Dict, Any
from bson import ObjectId 
from pydantic import BaseModel, Field, GetCoreSchemaHandler
from pydantic_core import core_schema

class PyObjectId(ObjectId):
    @classmethod
    def __get_validators__(cls):
        yield cls.validate

    @classmethod
    def validate(cls, v, info): 
        if not ObjectId.is_valid(v):
            raise ValueError("Invalid ObjectId format")
        return ObjectId(v)

    @classmethod
    def __get_pydantic_core_schema__(
        cls, source: type[Any], handler: GetCoreSchemaHandler
    ) -> core_schema.CoreSchema:
        """
        Custom Pydantic v2 schema generation for ObjectId.
        This tells Pydantic how to treat this type for serialization and validation.
        """
        return core_schema.json_or_python_schema(
            json_schema=core_schema.str_schema(), 
            python_schema=core_schema.is_instance_schema(ObjectId),
            serialization=core_schema.to_string_ser_schema(),
        )
    
class DocumentMetadataResponse(BaseModel):
    id: PyObjectId = Field(alias="_id") 
    filename: str # Name of the uploaded file
    original_size_kb: int # Size of the original file in kilobytes
    upload_date: datetime # Timestamp of when the document was uploaded
    num_chunks: int # Number of text chunks generated from the document
    status: str # Processing status (e.g., "processed")
    file_path: str # Internal path where the original file is stored

    class Config:
        allow_population_by_field_name = True
        json_encoders = {ObjectId: str}
        arbitrary_types_allowed = True

# --- User Query Request Model ---

class QueryRequest(BaseModel):
    query: str 


# --- System Query Response Model ---
class QueryResponse(BaseModel):
    response: str # The answer generated by the Large Language Model (LLM)
    source_documents: List[Dict[str, Any]] 
